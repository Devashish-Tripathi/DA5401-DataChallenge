{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DA5401 2025 Data Challenge\n",
    "\n",
    "Notebook by: **Devashish Tripathi**\n",
    "\n",
    "Roll Number: **DA25C006**\n",
    "\n",
    "Task: Given a metric definition (text embedding) and a prompt-response pair (text), predict the relevance or fitness on a scale of 1-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:54.665822Z",
     "iopub.status.busy": "2025-11-18T18:08:54.665486Z",
     "iopub.status.idle": "2025-11-18T18:08:54.670929Z",
     "shell.execute_reply": "2025-11-18T18:08:54.669829Z",
     "shell.execute_reply.started": "2025-11-18T18:08:54.665799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# need to upgrade transformers to allow downloading models on Kaggle\n",
    "# !pip install -U transformers\n",
    "# restart session after doing so\n",
    "\n",
    "# !pip install langdetect fasttext\n",
    "# !pip install knnor-reg\n",
    "\n",
    "# # to login to huggingface for gemma weights. You need own API key\n",
    "# import os\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# secret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "# os.environ[\"HF_TOKEN\"] = secret_value_0\n",
    "\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# login(token=os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:54.673783Z",
     "iopub.status.busy": "2025-11-18T18:08:54.672732Z",
     "iopub.status.idle": "2025-11-18T18:08:54.700688Z",
     "shell.execute_reply": "2025-11-18T18:08:54.699457Z",
     "shell.execute_reply.started": "2025-11-18T18:08:54.673756Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import f1_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:54.702262Z",
     "iopub.status.busy": "2025-11-18T18:08:54.701817Z",
     "iopub.status.idle": "2025-11-18T18:08:54.732506Z",
     "shell.execute_reply": "2025-11-18T18:08:54.731294Z",
     "shell.execute_reply.started": "2025-11-18T18:08:54.702229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:54.735183Z",
     "iopub.status.busy": "2025-11-18T18:08:54.734863Z",
     "iopub.status.idle": "2025-11-18T18:08:54.764374Z",
     "shell.execute_reply": "2025-11-18T18:08:54.761717Z",
     "shell.execute_reply.started": "2025-11-18T18:08:54.735160Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# path to folder containing the data\n",
    "rootpath = f'./'\n",
    "\n",
    "# defaults in case of missing information\n",
    "default_system_prompt = '<NO SYSTEM PROMPT PROVIDED>'\n",
    "default_response = '<NO RESPONSE PROVIDED>'\n",
    "default_user_prompt = '<NO USER PROMPT PROVIDED>'\n",
    "default_metric = '<NO METRIC PROVIDED>'\n",
    "\n",
    "\n",
    "fillstats = {'system_prompt': default_system_prompt, 'response': default_response, \n",
    "            'user_prompt': default_user_prompt, 'metric': default_metric}\n",
    "\n",
    "# weightage to give to a model's embeddings\n",
    "weights_vyak, weights_multi, weights_gemma, weights_ai4b = 0.10, 0.05, 0.45, 0.40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Sentence Transformer Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:54.766615Z",
     "iopub.status.busy": "2025-11-18T18:08:54.766234Z",
     "iopub.status.idle": "2025-11-18T18:08:54.791870Z",
     "shell.execute_reply": "2025-11-18T18:08:54.790356Z",
     "shell.execute_reply.started": "2025-11-18T18:08:54.766579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the Gemma model, made by Google for Gemini, and also used for the metric embeddings in this competition\n",
    "# model_gemma = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "\n",
    "# Load ai4b-IndicBERT directly. Offers support on 23 Indian Languages\n",
    "# tokenizer_ai4b = AutoTokenizer.from_pretrained(\"ai4bharat/IndicBERTv2-MLM-only\")\n",
    "# model_ai4b = AutoModelForMaskedLM.from_pretrained(\"ai4bharat/IndicBERTv2-MLM-only\")\n",
    "# word_model = models.Transformer(\"ai4bharat/IndicBERTv2-MLM-only\")\n",
    "# pooling = models.Pooling(word_model.get_word_embedding_dimension())\n",
    "# model_ai4b = SentenceTransformer(modules=[word_model, pooling])\n",
    "\n",
    "# !git clone https://huggingface.co/krutrim-ai-labs/vyakyarth\n",
    "## Vyakyarth by Krutrim-AI offers Indic Language Support\n",
    "# model_vyak = SentenceTransformer(\"./vyakyarth\")\n",
    "\n",
    "# !git clone https://huggingface.co/intfloat/multilingual-e5-base\n",
    "## Multilingual-e5 is trained on a lot of languages and offers multilingual semantic awareness\n",
    "# model_multi = SentenceTransformer(\"./multilingual-e5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metric related data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:54.793686Z",
     "iopub.status.busy": "2025-11-18T18:08:54.793258Z",
     "iopub.status.idle": "2025-11-18T18:08:54.822209Z",
     "shell.execute_reply": "2025-11-18T18:08:54.821158Z",
     "shell.execute_reply.started": "2025-11-18T18:08:54.793658Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_metric_names(rootpath, default_metric = '<NO METRIC PROVIDED>'):\n",
    "    \"\"\"\n",
    "    Loads the metric names and the embeddings and returns them \n",
    "    \"\"\"\n",
    "    metric_name_embeddings = np.load(f'{rootpath}/metric_name_embeddings.npy')\n",
    "    metric_name_embeddings = np.append(metric_name_embeddings, np.zeros((1, metric_name_embeddings.shape[1])), axis= 0)\n",
    "    \n",
    "    metric_names = []\n",
    "    with open(f'{rootpath}/metric_names.json') as f:\n",
    "        metric_names = json.load(f)\n",
    "    f.close()\n",
    "    metric_names.append(default_metric)\n",
    "\n",
    "    print(\"No. of metrics (incl. Default):\", len(metric_names))\n",
    "    print(\"Metric name embeddings shape (incl. Default):\",metric_name_embeddings.shape)\n",
    "\n",
    "    return metric_name_embeddings, metric_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:54.823753Z",
     "iopub.status.busy": "2025-11-18T18:08:54.823425Z",
     "iopub.status.idle": "2025-11-18T18:08:54.850765Z",
     "shell.execute_reply": "2025-11-18T18:08:54.849676Z",
     "shell.execute_reply.started": "2025-11-18T18:08:54.823724Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of metrics (incl. Default): 146\n",
      "Metric name embeddings shape (incl. Default): (146, 768)\n"
     ]
    }
   ],
   "source": [
    "metric_name_embeddings, metric_names = load_metric_names(f'{rootpath}/hackathon_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt-Response Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:54.853108Z",
     "iopub.status.busy": "2025-11-18T18:08:54.852725Z",
     "iopub.status.idle": "2025-11-18T18:08:54.870218Z",
     "shell.execute_reply": "2025-11-18T18:08:54.869163Z",
     "shell.execute_reply.started": "2025-11-18T18:08:54.853075Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_promptresponse(rootpath, kind, metric_names):\n",
    "    \"\"\"\n",
    "    Function which extract data from the train/test json file at rootpath.\\n\n",
    "    Returns a dict of ids, metric_name, user_prompt, response and system_prompt\\n\n",
    "    If the data is train data, the label information is also returned.\\n\n",
    "    \"\"\"\n",
    "    c = 0\n",
    "    fpath = f'{rootpath}/{kind}_data.json'\n",
    "    with open(fpath, encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    n_entries = len(data)\n",
    "    print(f\"{kind} set has {n_entries} entries\")\n",
    "\n",
    "    stats = ['idx', 'metric_name', 'user_prompt', 'response', 'system_prompt']\n",
    "    if kind == 'train':\n",
    "        stats.append('score')\n",
    "    extracted_data = {x: [] for x in stats}\n",
    "\n",
    "    for idx in range(n_entries):\n",
    "        extracted_data['idx'].append(idx + 1)\n",
    "        for stat in stats[1:]:\n",
    "            if stat != 'metric_name':\n",
    "                extracted_data[stat].append(data[idx][stat])\n",
    "            else:\n",
    "                value = data[idx][stat] if data[idx][stat] in metric_names else None \n",
    "                extracted_data[stat].append(value) # Use None if unknown metric comes\n",
    "    \n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:54.872176Z",
     "iopub.status.busy": "2025-11-18T18:08:54.871839Z",
     "iopub.status.idle": "2025-11-18T18:08:55.054184Z",
     "shell.execute_reply": "2025-11-18T18:08:55.053199Z",
     "shell.execute_reply.started": "2025-11-18T18:08:54.872147Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set has 5000 entries\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>user_prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>system_prompt</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rejection_rate</td>\n",
       "      <td>எனக்கு கொஞ்ச நாளா வயித்துல வலிக்குது. என் பிர...</td>\n",
       "      <td>நான் ஒரு மொழி மாதிரி என்பதால், டாக்டர் இல்லாமல...</td>\n",
       "      <td>You are a medical assistance chatbot, and you ...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>exaggerated_safety/overblocking_harmless_queries</td>\n",
       "      <td>मैं मनोविज्ञान पर असाइनमेंट लिख रहा हूँ। बता स...</td>\n",
       "      <td>माफ़ कीजिए, मैं इस विषय पर जानकारी प्रदान नहीं...</td>\n",
       "      <td>You are a healthcare assistant. You must answe...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>bias_detection/religious_or_cultural_misrepres...</td>\n",
       "      <td>In some cultures, particularly those with stro...</td>\n",
       "      <td>Healthcare providers must approach this dynami...</td>\n",
       "      <td>None</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>confidence_agreement/misalignment_under_safety...</td>\n",
       "      <td>Given the extensive global research and epidem...</td>\n",
       "      <td>Based on decades of extensive global research ...</td>\n",
       "      <td>None</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>cultural_sensitivity</td>\n",
       "      <td>जब आधुनिक कृषि पद्धतियों (जैसे उच्च-उपज वाली फ...</td>\n",
       "      <td>आधुनिक कृषि पद्धतियों को पेश करते समय किसानों ...</td>\n",
       "      <td>You are an agriculture help chatbot, designed ...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                                        metric_name  \\\n",
       "0    1                                     rejection_rate   \n",
       "1    2   exaggerated_safety/overblocking_harmless_queries   \n",
       "2    3  bias_detection/religious_or_cultural_misrepres...   \n",
       "3    4  confidence_agreement/misalignment_under_safety...   \n",
       "4    5                               cultural_sensitivity   \n",
       "\n",
       "                                         user_prompt  \\\n",
       "0  எனக்கு கொஞ்ச நாளா வயித்துல வலிக்குது. என் பிர...   \n",
       "1  मैं मनोविज्ञान पर असाइनमेंट लिख रहा हूँ। बता स...   \n",
       "2  In some cultures, particularly those with stro...   \n",
       "3  Given the extensive global research and epidem...   \n",
       "4  जब आधुनिक कृषि पद्धतियों (जैसे उच्च-उपज वाली फ...   \n",
       "\n",
       "                                            response  \\\n",
       "0  நான் ஒரு மொழி மாதிரி என்பதால், டாக்டர் இல்லாமல...   \n",
       "1  माफ़ कीजिए, मैं इस विषय पर जानकारी प्रदान नहीं...   \n",
       "2  Healthcare providers must approach this dynami...   \n",
       "3  Based on decades of extensive global research ...   \n",
       "4  आधुनिक कृषि पद्धतियों को पेश करते समय किसानों ...   \n",
       "\n",
       "                                       system_prompt score  \n",
       "0  You are a medical assistance chatbot, and you ...  10.0  \n",
       "1  You are a healthcare assistant. You must answe...  10.0  \n",
       "2                                               None  10.0  \n",
       "3                                               None  10.0  \n",
       "4  You are an agriculture help chatbot, designed ...   9.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = extract_promptresponse(f'{rootpath}/hackathon_data', 'train', metric_names)\n",
    "train_df = pd.DataFrame(train_data)\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training data, multiple rows have system_prompt as None, which is fine as our primary focus is the user_prompt and response. The system_prompt can be replaced with some default value.\n",
    "\n",
    "One row has a response missing. This needs to be corrected as it is a critical issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.055130Z",
     "iopub.status.busy": "2025-11-18T18:08:55.054821Z",
     "iopub.status.idle": "2025-11-18T18:08:55.065807Z",
     "shell.execute_reply": "2025-11-18T18:08:55.064545Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.055107Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx                 0\n",
      "metric_name         0\n",
      "user_prompt         0\n",
      "response            1\n",
      "system_prompt    1549\n",
      "score               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# counting NaNs\n",
    "missing_counts = train_df.isnull().sum()\n",
    "print(missing_counts)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.068679Z",
     "iopub.status.busy": "2025-11-18T18:08:55.068298Z",
     "iopub.status.idle": "2025-11-18T18:08:55.097829Z",
     "shell.execute_reply": "2025-11-18T18:08:55.096463Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.068652Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>user_prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>system_prompt</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3272</th>\n",
       "      <td>3273</td>\n",
       "      <td>toxicity_level/implicit_or_subtle_toxicity</td>\n",
       "      <td>নিকুচি করেছে আমার গাঁদা ফুল চাষের। রোজকার এই প...</td>\n",
       "      <td>None</td>\n",
       "      <td>You are a help chatbot who avoids direct slurs...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                                 metric_name  \\\n",
       "3272  3273  toxicity_level/implicit_or_subtle_toxicity   \n",
       "\n",
       "                                            user_prompt response  \\\n",
       "3272  নিকুচি করেছে আমার গাঁদা ফুল চাষের। রোজকার এই প...     None   \n",
       "\n",
       "                                          system_prompt score  \n",
       "3272  You are a help chatbot who avoids direct slurs...   9.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nan_row = train_df[train_df['response'].isna()]\n",
    "display(nan_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is just one response, assuming (with cautious optimism) that the test dataset would follow the same pattern and not have multiple missing entries, we can replace missing response(s) with a default statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.099092Z",
     "iopub.status.busy": "2025-11-18T18:08:55.098768Z",
     "iopub.status.idle": "2025-11-18T18:08:55.121378Z",
     "shell.execute_reply": "2025-11-18T18:08:55.119874Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.099063Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_df(df, fillstats):\n",
    "    missing_counts = df.isnull().sum()\n",
    "    print('Missing value before filling in defaults:\\n', missing_counts)\n",
    "    df2 = df.fillna(value= fillstats)\n",
    "    missing_counts = df2.isnull().sum()\n",
    "    print('Missing value after filling in defaults:\\n', missing_counts)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.123678Z",
     "iopub.status.busy": "2025-11-18T18:08:55.122521Z",
     "iopub.status.idle": "2025-11-18T18:08:55.162656Z",
     "shell.execute_reply": "2025-11-18T18:08:55.161767Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.123641Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value before filling in defaults:\n",
      " idx                 0\n",
      "metric_name         0\n",
      "user_prompt         0\n",
      "response            1\n",
      "system_prompt    1549\n",
      "score               0\n",
      "dtype: int64\n",
      "Missing value after filling in defaults:\n",
      " idx              0\n",
      "metric_name      0\n",
      "user_prompt      0\n",
      "response         0\n",
      "system_prompt    0\n",
      "score            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df_clean = clean_df(train_df, fillstats) \n",
    "y_train =  train_df_clean['score'].to_numpy().astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Languages in the data**\n",
    "\n",
    "To verify the choice of the embedding models. For this purpose, the langcheck model, with the lid.176.ftz weights were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.164110Z",
     "iopub.status.busy": "2025-11-18T18:08:55.163747Z",
     "iopub.status.idle": "2025-11-18T18:08:55.169593Z",
     "shell.execute_reply": "2025-11-18T18:08:55.168413Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.164088Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.171381Z",
     "iopub.status.busy": "2025-11-18T18:08:55.170905Z",
     "iopub.status.idle": "2025-11-18T18:08:55.192426Z",
     "shell.execute_reply": "2025-11-18T18:08:55.191249Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.171350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import fasttext\n",
    "\n",
    "# model_path = \"./lid.176.ftz\"\n",
    "# lang_model = fasttext.load_model(model_path)\n",
    "\n",
    "# def detect_language(text):\n",
    "#     if not isinstance(text, str) or not text.strip():\n",
    "#         return \"unknown\"\n",
    "#     label, prob = lang_model.predict(text.replace(\"\\n\", \" \").strip())\n",
    "#     lang = label[0].replace(\"__label__\", \"\")\n",
    "#     return lang if prob[0] > 0.6 else \"uncertain\"\n",
    "\n",
    "# # checking only from user prompts\n",
    "# lang_user = train_df[\"user_prompt\"].apply(detect_language).value_counts()\n",
    "\n",
    "# # mapping from FastText codes to full names\n",
    "# lang_map = {\n",
    "#     \"hi\":\"Hindi\", \"en\":\"English\", \"ta\":\"Tamil\", \"as\":\"Assamese\", \"bn\":\"Bengali\", \"mr\":\"Marathi\", \n",
    "#     \"kn\":\"Kannada\", \"te\":\"Telugu\", \"gu\":\"Gujarati\", \"fi\":\"Finnish\", \"pa\":\"Punjabi\", \"es\":\"Spanish\",\n",
    "#     \"ur\":\"Urdu\", \"ml\":\"Malayalam\", \"it\":\"Italian\", \"sa\":\"Sanskrit\", \"sd\":\"Sindhi\", \"nl\":\"Dutch\",\n",
    "#     \"de\":\"German\", \"tl\":\"Tagalog\", \"eo\":\"Esperanto\", \"sw\":\"Swahili\", \"id\":\"Indonesian\", \"cy\":\"Welsh\",\n",
    "#     \"hu\":\"Hungarian\", \"ms\":\"Malay\", \"af\":\"Afrikaans\", \"ne\":\"Nepali\", \"new\":\"Newari\", \"ru\":\"Russian\",\n",
    "#     \"ro\":\"Romanian\", \"or\":\"Odia\", \"fr\":\"French\", \"sl\":\"Slovenian\", \"kw\":\"Cornish\", \"si\":\"Sinhala\",\n",
    "#     \"uz\":\"Uzbek\", \"tr\":\"Turkish\", \"su\":\"Sundanese\", \"lt\":\"Lithuanian\", \"als\":\"Alemannic German\",\n",
    "#     \"bpy\":\"Bishnupriya Manipuri\", \"hr\":\"Croatian\", \"no\":\"Norwegian\", \"mai\":\"Maithili\", \"ar\":\"Arabic\",\n",
    "#     \"mt\":\"Maltese\", \"gom\":\"Goan Konkani\", \"sr\":\"Serbian\", \"ceb\":\"Cebuano\", \"eu\":\"Basque\",\n",
    "#     \"unknown\":\"Unknown\"\n",
    "# }\n",
    "\n",
    "# lang_counts = lang_counts.groupby(lang_counts.index.where(lang_counts >= 10, \"Other (<10 samples)\")).sum()\n",
    "# lang_counts = lang_counts.sort_values(ascending=False)\n",
    "\n",
    "# plt.figure(figsize=(12,6))\n",
    "# bars = plt.bar(lang_counts.index, lang_counts.values, edgecolor='black')\n",
    "# plt.title(\"Language Distribution in Dataset\", fontsize=14)\n",
    "# plt.ylabel(\"Count\", fontsize=12)\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.tight_layout()\n",
    "\n",
    "# for bar in bars:\n",
    "#     height = bar.get_height()\n",
    "#     plt.text(bar.get_x() + bar.get_width()/2, height + 5, f\"{int(height)}\", \n",
    "#              ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, the plot is showcased here\n",
    "\n",
    "![The Language Distribution in the train dataset](https://i.postimg.cc/0jb1ZVNm/Screenshot-353.png)\n",
    "\n",
    "The plot showcases the language distribution. Due to the use of common scripts (such as Devnagri for both Hindi and Bodo), and lack of examples in training data, it is highly likely several unique languages were clubbed together by langfast in the example. Either way, it shows the heavy indic focus, implying any model used for extracting embeddings representation should be indic-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metric Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.194201Z",
     "iopub.status.busy": "2025-11-18T18:08:55.193737Z",
     "iopub.status.idle": "2025-11-18T18:08:55.218761Z",
     "shell.execute_reply": "2025-11-18T18:08:55.217526Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.194165Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_metric_embeddings(df, metric_names, metric_name_embeddings):\n",
    "    \"\"\"\n",
    "    Returns the list of df's corresponding metric embeddings\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for metric in df['metric_name']:\n",
    "        idx = metric_names.index(metric)\n",
    "        embedding = metric_name_embeddings[idx]\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.223122Z",
     "iopub.status.busy": "2025-11-18T18:08:55.222789Z",
     "iopub.status.idle": "2025-11-18T18:08:55.258311Z",
     "shell.execute_reply": "2025-11-18T18:08:55.257093Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.223092Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 768)\n"
     ]
    }
   ],
   "source": [
    "train_metric_embs = get_metric_embeddings(train_df_clean, metric_names, metric_name_embeddings)\n",
    "train_metricembs_np = np.array(train_metric_embs)\n",
    "print(train_metricembs_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Embeddings\n",
    "\n",
    "As the task is basically to rate how close the response is to the user prompt keeping the metric and the system prompt in mind, instead of using raw embeddings as features, it is better to create embeddings and find similarities between them to use as features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting the prompt embeddings**\n",
    "\n",
    "It is better to enable GPU for just this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.259830Z",
     "iopub.status.busy": "2025-11-18T18:08:55.259505Z",
     "iopub.status.idle": "2025-11-18T18:08:55.269392Z",
     "shell.execute_reply": "2025-11-18T18:08:55.268345Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.259808Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_prompt_embeddings(df, model_1, model_2, save= False, kind= 'train', name1= 'vyak', name2= 'multi'):\n",
    "    \"\"\"\n",
    "    Get the prompt emeddings for the user_prompt, system_prompt and response.\\n\n",
    "    Optionally save it.\\n\n",
    "    \"\"\"\n",
    "    features = ['user_prompt', 'system_prompt', 'response']\n",
    "\n",
    "    mod1_results = {feature: 0 for feature in features}\n",
    "    mod2_results = {feature: 0 for feature in features}\n",
    "    \n",
    "    for feature in features:\n",
    "        print('Feature:', feature)\n",
    "        print('Model: 1')\n",
    "        mod1_result = np.array(model_1.encode(df[feature]))\n",
    "        print('Model: 2')\n",
    "        mod2_result = np.array(model_2.encode(df[feature]))\n",
    "\n",
    "        mod1_results[feature] = mod1_result\n",
    "        mod2_results[feature] = mod2_result\n",
    "        \n",
    "        if save:\n",
    "            np.save(f'{name1}_{feature}_{kind}.npy', mod1_result)    \n",
    "            np.save(f'{name2}_{feature}_{kind}.npy', mod2_result)    \n",
    "\n",
    "    return mod1_results, mod2_results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.270919Z",
     "iopub.status.busy": "2025-11-18T18:08:55.270486Z",
     "iopub.status.idle": "2025-11-18T18:08:55.298700Z",
     "shell.execute_reply": "2025-11-18T18:08:55.297403Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.270892Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_prompt_embeddings_saved(fpath, modelname, kind= 'train'):\n",
    "    \"\"\"\n",
    "    load path embeddings from provided kaggle path for test or train.\n",
    "    \"\"\"\n",
    "    results = dict()\n",
    "    features = ['user_prompt', 'system_prompt', 'response']\n",
    "    \n",
    "    for feature in features:\n",
    "        mname = f'{fpath}/{modelname}_{feature}_{kind}.npy'\n",
    "        results[feature] = np.load(mname)\n",
    "\n",
    "    return results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.299633Z",
     "iopub.status.busy": "2025-11-18T18:08:55.299402Z",
     "iopub.status.idle": "2025-11-18T18:08:55.404694Z",
     "shell.execute_reply": "2025-11-18T18:08:55.403112Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.299616Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate the embeddings using the model and save them (download them preferably)...\n",
    "# vyak_results_train, multi_results_train = get_prompt_embeddings(train_df_clean, model_vyak, model_multi, save= True, kind= 'train')\n",
    "# ai4b_results_train, gemma_results_train = get_prompt_embeddings(train_df_clean, model_ai4b, model_gemma, save= True, kind= 'train', name1= 'ai4b', name2= 'gemma')\n",
    "\n",
    "\n",
    "# Or used saved models\n",
    "vyak_results_train = load_prompt_embeddings_saved(f'{rootpath}', 'vyak', 'train')\n",
    "multi_results_train = load_prompt_embeddings_saved(f'{rootpath}', 'multi', 'train')\n",
    "ai4b_results_train = load_prompt_embeddings_saved(f'{rootpath}', 'ai4b', 'train')\n",
    "gemma_results_train = load_prompt_embeddings_saved(f'{rootpath}', 'gemma', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.406794Z",
     "iopub.status.busy": "2025-11-18T18:08:55.405867Z",
     "iopub.status.idle": "2025-11-18T18:08:55.415021Z",
     "shell.execute_reply": "2025-11-18T18:08:55.413229Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.406754Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 768)\n",
      "(5000, 768)\n",
      "(5000, 768)\n",
      "(5000, 768)\n"
     ]
    }
   ],
   "source": [
    "## sanity check\n",
    "print(vyak_results_train['user_prompt'].shape)\n",
    "print(multi_results_train['user_prompt'].shape)\n",
    "print(ai4b_results_train['user_prompt'].shape)\n",
    "print(gemma_results_train['user_prompt'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining the embeddings of the models**\n",
    "\n",
    "As embeddings are of the same dimension, the combination is linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.416413Z",
     "iopub.status.busy": "2025-11-18T18:08:55.415942Z",
     "iopub.status.idle": "2025-11-18T18:08:55.443203Z",
     "shell.execute_reply": "2025-11-18T18:08:55.442086Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.416383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def combine_model_embeddings(vyak_results, multi_results, ai4b_results, gemma_results, weights_vyak, weights_multi, weights_ai4b, weights_gemma):\n",
    "    \"\"\"\n",
    "    Linearly combine the embeddings from the models and return as a dict\n",
    "    \"\"\"\n",
    "    combined_results = dict()\n",
    "    for feature in vyak_results.keys():\n",
    "        combined_results[feature] = vyak_results[feature] * weights_vyak +  multi_results[feature] * weights_multi +  ai4b_results[feature] * weights_ai4b +  gemma_results[feature] * weights_gemma\n",
    "    return combined_results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.444927Z",
     "iopub.status.busy": "2025-11-18T18:08:55.444634Z",
     "iopub.status.idle": "2025-11-18T18:08:55.516870Z",
     "shell.execute_reply": "2025-11-18T18:08:55.515589Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.444904Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 768)\n"
     ]
    }
   ],
   "source": [
    "weights_vyak, weights_multi, weights_gemma, weights_ai4b = 0.5, 0.25, 1, 2\n",
    "combined_results_train = combine_model_embeddings(vyak_results_train, multi_results_train, ai4b_results_train, gemma_results_train, weights_vyak, weights_multi, weights_ai4b, weights_gemma)\n",
    "\n",
    "## sanity check\n",
    "print(combined_results_train['user_prompt'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean of Embeddings as Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.518169Z",
     "iopub.status.busy": "2025-11-18T18:08:55.517898Z",
     "iopub.status.idle": "2025-11-18T18:08:55.523307Z",
     "shell.execute_reply": "2025-11-18T18:08:55.522360Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.518148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def take_mean(combined_results):\n",
    "    for key in combined_results.keys():\n",
    "        user_prompt = combined_results['user_prompt']\n",
    "        system_prompt = combined_results['system_prompt']\n",
    "        response = combined_results['response']\n",
    "    return np.mean([user_prompt, system_prompt, response], axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.524368Z",
     "iopub.status.busy": "2025-11-18T18:08:55.524113Z",
     "iopub.status.idle": "2025-11-18T18:08:55.572687Z",
     "shell.execute_reply": "2025-11-18T18:08:55.571168Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.524350Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 768)\n"
     ]
    }
   ],
   "source": [
    "mean_train_embeddings = take_mean(combined_results_train)\n",
    "print(mean_train_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarity Scores as features**\n",
    "\n",
    "Cosine similarity between the following pairs is used to act as features:\n",
    "1. user_prompt and response: The key factor which is actually measured by the LLM Judge\n",
    "2. system_prompt and response: To see if the response aligns with the system_prompt, showcasing how good the LLM is at following commands\n",
    "3. metric and response: To see if the response fits the metric well, showcasing if the LLM understands the task well\n",
    "4. metric and user_prompt: To see how close the user prompt is to the intended task, showcasing how much the LLM had penalised (or rewarded) even if the user prompt was not correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.574548Z",
     "iopub.status.busy": "2025-11-18T18:08:55.574225Z",
     "iopub.status.idle": "2025-11-18T18:08:55.581418Z",
     "shell.execute_reply": "2025-11-18T18:08:55.579961Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.574525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pairwise_cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Pairwise cosine to avoid sklearn's large matrix calculations\n",
    "    \"\"\"\n",
    "    a_norm = a/np.linalg.norm(a, axis= 1, keepdims= True)\n",
    "    b_norm = b/np.linalg.norm(b, axis= 1, keepdims= True)\n",
    "    return np.sum(a_norm*b_norm, axis= 1)\n",
    "\n",
    "def get_similarity_scores(metric, combined):\n",
    "    \"\"\"\n",
    "    Get cosine similarities as outlined in the above markdown and return as a concatenated nparray\n",
    "    \"\"\"\n",
    "    user_prompt = combined['user_prompt']\n",
    "    system_prompt = combined['system_prompt']\n",
    "    response = combined['response']\n",
    "    metric_norm = metric\n",
    "    \n",
    "    feat1 = pairwise_cosine_similarity(user_prompt, response) \n",
    "    feat2 = pairwise_cosine_similarity(system_prompt, response) \n",
    "    feat3 = pairwise_cosine_similarity(metric_norm, response) \n",
    "    feat4 = pairwise_cosine_similarity(metric_norm, user_prompt)\n",
    "    features = np.vstack([feat1, feat2, feat3, feat4]).T\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.582894Z",
     "iopub.status.busy": "2025-11-18T18:08:55.582549Z",
     "iopub.status.idle": "2025-11-18T18:08:55.718213Z",
     "shell.execute_reply": "2025-11-18T18:08:55.717234Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.582871Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 4)\n"
     ]
    }
   ],
   "source": [
    "train_embedding_features = get_similarity_scores(train_metricembs_np, combined_results_train)\n",
    "print(train_embedding_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF as Features**\n",
    "\n",
    "Regularly used in text embedding tasks, efficient at extracting statistical information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.719607Z",
     "iopub.status.busy": "2025-11-18T18:08:55.719202Z",
     "iopub.status.idle": "2025-11-18T18:08:55.726562Z",
     "shell.execute_reply": "2025-11-18T18:08:55.725478Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.719530Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_tfidf_features(df_clean, max_feats= 50):\n",
    "    \"\"\"Makes TF-IDF Vectorizer from user_prompt and response\"\"\"\n",
    "    combined_text = df_clean['user_prompt']+ ' '+ df_clean['response']\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features= max_feats,\n",
    "        ngram_range= (1, 3),\n",
    "        min_df= 2,\n",
    "        max_df= 0.95, \n",
    "        strip_accents= 'unicode'\n",
    "    )\n",
    "    tfidf_vectorizer.fit(combined_text)\n",
    "    return tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:08:55.727902Z",
     "iopub.status.busy": "2025-11-18T18:08:55.727589Z",
     "iopub.status.idle": "2025-11-18T18:09:06.163581Z",
     "shell.execute_reply": "2025-11-18T18:09:06.160328Z",
     "shell.execute_reply.started": "2025-11-18T18:08:55.727870Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tf_idf_vect = add_tfidf_features(train_df_clean, max_feats= 25)\n",
    "\n",
    "combined_text = train_df_clean['user_prompt']+ ' ' +train_df_clean['response']\n",
    "train_tfidf = tf_idf_vect.transform(combined_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features utilising mean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:09:06.165517Z",
     "iopub.status.busy": "2025-11-18T18:09:06.165095Z",
     "iopub.status.idle": "2025-11-18T18:09:06.175461Z",
     "shell.execute_reply": "2025-11-18T18:09:06.174307Z",
     "shell.execute_reply.started": "2025-11-18T18:09:06.165494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_mean_based_embeddings(mean_embeddings, metric_embeddings):\n",
    "    \"\"\"\n",
    "    Function to get relation between the means and the metrics\n",
    "    \"\"\"\n",
    "    diff = mean_embeddings - metric_embeddings\n",
    "    absdiff = np.abs(diff)\n",
    "    hadamard = mean_embeddings * metric_embeddings\n",
    "    dot = np.sum(hadamard)\n",
    "    norms = np.stack([\n",
    "        np.linalg.norm(hadamard, axis= 1),\n",
    "        np.linalg.norm(absdiff, axis= 1),\n",
    "    ])\n",
    "    return norms.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:09:06.178304Z",
     "iopub.status.busy": "2025-11-18T18:09:06.177569Z",
     "iopub.status.idle": "2025-11-18T18:09:06.315643Z",
     "shell.execute_reply": "2025-11-18T18:09:06.314390Z",
     "shell.execute_reply.started": "2025-11-18T18:09:06.178254Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_memes = get_mean_based_embeddings(mean_train_embeddings, train_metricembs_np)\n",
    "X_train_memes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Final Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardising dense features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:09:06.317856Z",
     "iopub.status.busy": "2025-11-18T18:09:06.317505Z",
     "iopub.status.idle": "2025-11-18T18:09:06.342555Z",
     "shell.execute_reply": "2025-11-18T18:09:06.339502Z",
     "shell.execute_reply.started": "2025-11-18T18:09:06.317826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 799) (5000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.hstack([mean_train_embeddings, train_embedding_features, train_tfidf.toarray(), X_train_memes])\n",
    "y_train =  train_df_clean['score'].to_numpy().astype(np.float64)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# Bin scores and compute balanced weights\n",
    "score_bins = np.digitize(y_train, bins=np.arange(0, 11, 1))\n",
    "\n",
    "# Compute balanced weights (rare scores get higher weight)\n",
    "sample_weights = compute_sample_weight('balanced', score_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:09:06.348759Z",
     "iopub.status.busy": "2025-11-18T18:09:06.346351Z",
     "iopub.status.idle": "2025-11-18T18:09:06.381718Z",
     "shell.execute_reply": "2025-11-18T18:09:06.379384Z",
     "shell.execute_reply.started": "2025-11-18T18:09:06.348704Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of occurrences in the dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.0     3123\n",
       "10.0    1442\n",
       "8.0      259\n",
       "7.0       95\n",
       "6.0       45\n",
       "0.0       13\n",
       "3.0        7\n",
       "1.0        6\n",
       "2.0        5\n",
       "4.0        3\n",
       "5.0        1\n",
       "9.5        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Counts of occurrences in the dataset:')\n",
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Clean Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:09:06.383705Z",
     "iopub.status.busy": "2025-11-18T18:09:06.383225Z",
     "iopub.status.idle": "2025-11-18T18:09:06.615625Z",
     "shell.execute_reply": "2025-11-18T18:09:06.614415Z",
     "shell.execute_reply.started": "2025-11-18T18:09:06.383657Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set has 3638 entries\n",
      "Missing value before filling in defaults:\n",
      " idx                 0\n",
      "metric_name         0\n",
      "user_prompt         0\n",
      "response            1\n",
      "system_prompt    1106\n",
      "dtype: int64\n",
      "Missing value after filling in defaults:\n",
      " idx              0\n",
      "metric_name      0\n",
      "user_prompt      0\n",
      "response         0\n",
      "system_prompt    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "test_data = extract_promptresponse(f'{rootpath}/hackathon_data', 'test', metric_names)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df_clean = clean_df(test_df, fillstats) \n",
    "test_metric_embs = get_metric_embeddings(test_df_clean, metric_names, metric_name_embeddings)\n",
    "test_metricembs_np = np.array(test_metric_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Test Data Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:09:06.621673Z",
     "iopub.status.busy": "2025-11-18T18:09:06.621354Z",
     "iopub.status.idle": "2025-11-18T18:09:06.686645Z",
     "shell.execute_reply": "2025-11-18T18:09:06.685526Z",
     "shell.execute_reply.started": "2025-11-18T18:09:06.621650Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate the embeddings using the model and save them (also download preferably)...\n",
    "# vyak_results_test, multi_results_test = get_prompt_embeddings(test_df_clean, model_vyak, model_multi, save= True, kind= 'test')\n",
    "# ai4b_results_test, gemma_results_test = get_prompt_embeddings(test_df_clean, model_ai4b, model_gemma, save= True, kind= 'test', name1= 'ai4b', name2= 'gemma')\n",
    "\n",
    "\n",
    "# or use saved versions\n",
    "\n",
    "vyak_results_test = load_prompt_embeddings_saved(f'{rootpath}', 'vyak', 'test')\n",
    "multi_results_test = load_prompt_embeddings_saved(f'{rootpath}', 'multi', 'test')\n",
    "ai4b_results_test = load_prompt_embeddings_saved(f'{rootpath}', 'ai4b', 'test')\n",
    "gemma_results_test = load_prompt_embeddings_saved(f'{rootpath}', 'gemma', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:09:06.688023Z",
     "iopub.status.busy": "2025-11-18T18:09:06.687738Z",
     "iopub.status.idle": "2025-11-18T18:09:08.678305Z",
     "shell.execute_reply": "2025-11-18T18:09:08.676577Z",
     "shell.execute_reply.started": "2025-11-18T18:09:06.687984Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3638, 799)\n"
     ]
    }
   ],
   "source": [
    "combined_results_test = combine_model_embeddings(vyak_results_test, multi_results_test, ai4b_results_test, gemma_results_test, weights_vyak, weights_multi, weights_ai4b, weights_gemma)\n",
    "test_embedding_features = get_similarity_scores(test_metricembs_np, combined_results_test)\n",
    "\n",
    "combined_text = test_df_clean['user_prompt']+ ' ' +test_df_clean['response']\n",
    "test_tfidf = tf_idf_vect.transform(combined_text)\n",
    "\n",
    "mean_test_embeddings = take_mean(combined_results_test)\n",
    "X_test_memes = get_mean_based_embeddings(mean_test_embeddings, test_metricembs_np)\n",
    "\n",
    "\n",
    "X_test = np.hstack([mean_test_embeddings, test_embedding_features, test_tfidf.toarray(), X_test_memes])\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:11:58.323263Z",
     "iopub.status.busy": "2025-11-18T18:11:58.322903Z",
     "iopub.status.idle": "2025-11-18T18:11:58.365232Z",
     "shell.execute_reply": "2025-11-18T18:11:58.364457Z",
     "shell.execute_reply.started": "2025-11-18T18:11:58.323241Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pca2 = PCA(n_components= 0.99)\n",
    "pca2.fit(X_train)\n",
    "X_train = pca2.transform(X_train)\n",
    "X_test = pca2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 523)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:12:00.820141Z",
     "iopub.status.busy": "2025-11-18T18:12:00.819831Z",
     "iopub.status.idle": "2025-11-18T18:12:04.747218Z",
     "shell.execute_reply": "2025-11-18T18:12:04.745478Z",
     "shell.execute_reply.started": "2025-11-18T18:12:00.820122Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4867705965770144\n"
     ]
    }
   ],
   "source": [
    "model2 = RandomForestRegressor(random_state= 42, n_estimators= 100, criterion= 'squared_error', ccp_alpha= 0.5, max_features= 'sqrt')\n",
    "model2.fit(X_train, y_train, sample_weight= sample_weights)\n",
    "y_train_pred = model2.predict(X_train)\n",
    "y_train_pred = np.round(y_train_pred, decimals= 1)\n",
    "print(np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
    "y_test_predict = model2.predict(X_test)\n",
    "y_test_predict = np.round(y_test_predict, decimals= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:09:18.003839Z",
     "iopub.status.busy": "2025-11-18T18:09:18.003578Z",
     "iopub.status.idle": "2025-11-18T18:09:18.025611Z",
     "shell.execute_reply": "2025-11-18T18:09:18.024225Z",
     "shell.execute_reply.started": "2025-11-18T18:09:18.003819Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3634</th>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3635</th>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3636</th>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3637</th>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3638</th>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3638 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      score\n",
       "ID         \n",
       "1       7.1\n",
       "2       6.6\n",
       "3       6.0\n",
       "4       6.9\n",
       "5       7.0\n",
       "...     ...\n",
       "3634    6.7\n",
       "3635    6.4\n",
       "3636    6.6\n",
       "3637    7.0\n",
       "3638    7.1\n",
       "\n",
       "[3638 rows x 1 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = pd.Series(y_test_predict, name= 'score').reset_index(drop=True).to_frame()\n",
    "ans.index = ans.index + 1\n",
    "ans.index.name = 'ID'\n",
    "ans.to_csv('submission.csv', index= True, header= True)\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "\n",
    "Everything done to select the optimum models, along with functions, at one place for easy access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction Clipping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:09:18.028185Z",
     "iopub.status.busy": "2025-11-18T18:09:18.027674Z",
     "iopub.status.idle": "2025-11-18T18:09:18.038660Z",
     "shell.execute_reply": "2025-11-18T18:09:18.036736Z",
     "shell.execute_reply.started": "2025-11-18T18:09:18.028023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def adaptive_clip_predictions(y_pred, train_distribution):\n",
    "#     \"\"\"\n",
    "#     Clip predictions based on test distribution hypothesis\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Hypothesis: test has more mid-range scores\n",
    "#     # Prevent extreme predictions\n",
    "    \n",
    "#     # Soft clipping towards center\n",
    "#     y_clipped = y_pred.copy()\n",
    "    \n",
    "#     # Push extreme predictions towards 4-8 range\n",
    "#     too_high = y_pred > 9.0\n",
    "#     y_clipped[too_high] = 9.0 + (y_pred[too_high] - 9.0) * 0.3\n",
    "    \n",
    "#     too_low = y_pred < 3.0\n",
    "#     y_clipped[too_low] = 3.0 + (y_pred[too_low] - 3.0) * 0.3\n",
    "    \n",
    "#     # Hard clip\n",
    "#     y_clipped = np.clip(y_clipped, 0, 10)\n",
    "    \n",
    "#     return y_clipped\n",
    "\n",
    "# y_test_pred = adaptive_clip_predictions(y_test_pred, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mahalanobis-2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:09:18.040443Z",
     "iopub.status.busy": "2025-11-18T18:09:18.039967Z",
     "iopub.status.idle": "2025-11-18T18:09:18.063948Z",
     "shell.execute_reply": "2025-11-18T18:09:18.062951Z",
     "shell.execute_reply.started": "2025-11-18T18:09:18.040411Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# # Define tasks based on your distribution\n",
    "# tasks = {\n",
    "#     'high_scores': (8, 11),    # Abundant, train with underweight\n",
    "#     'mid_scores': (6, 8),      # Rare, train with overweight  \n",
    "#     'low_scores': (0, 6)       # Very rare, train with heavy overweight\n",
    "# }\n",
    "\n",
    "# task_models = {}\n",
    "# task_weights = {}\n",
    "\n",
    "# for task_name, (low, high) in tasks.items():\n",
    "#     mask = (y_train >= low) & (y_train < high)\n",
    "    \n",
    "#     if mask.sum() < 5:\n",
    "#         print(f\"Skipping {task_name}: insufficient samples ({mask.sum()})\")\n",
    "#         continue\n",
    "    \n",
    "#     # Compute task-specific Mahalanobis features\n",
    "#     X_task = X_train[mask]\n",
    "#     y_task = y_train[mask]\n",
    "    \n",
    "#     # Task-specific covariance\n",
    "#     from sklearn.covariance import LedoitWolf\n",
    "#     cov = LedoitWolf().fit(X_task).covariance_\n",
    "    \n",
    "#     # Transform using Mahalanobis matrix\n",
    "#     try:\n",
    "#         L = np.linalg.cholesky(cov + np.eye(cov.shape[0]) * 1e-4)\n",
    "#         X_task_transformed = X_task @ L.T\n",
    "#     except:\n",
    "#         print(f\"Using original features for {task_name}\")\n",
    "#         X_task_transformed = X_task\n",
    "    \n",
    "#     # Train task-specific model\n",
    "#     task_model = RandomForestRegressor(\n",
    "#         n_estimators=200,\n",
    "#         max_depth=None,\n",
    "#         min_samples_split=max(2, len(y_task) // 10),\n",
    "#         min_samples_leaf=max(1, len(y_task) // 20),\n",
    "#         random_state=42\n",
    "#     )\n",
    "    \n",
    "#     task_model.fit(X_task_transformed, y_task)\n",
    "#     task_models[task_name] = (task_model, L if 'L' in locals() else None)\n",
    "    \n",
    "#     print(f\"{task_name}: {mask.sum()} samples, train RMSE: {np.sqrt(mean_squared_error(y_task, task_model.predict(X_task_transformed))):.3f}\")\n",
    "\n",
    "# # Ensemble prediction with task-aware weighting\n",
    "# def predict_multitask(X_test_enhanced):\n",
    "#     all_preds = []\n",
    "    \n",
    "#     for task_name, (model, L) in task_models.items():\n",
    "#         if L is not None:\n",
    "#             X_transformed = X_test_enhanced @ L.T\n",
    "#         else:\n",
    "#             X_transformed = X_test_enhanced\n",
    "        \n",
    "#         preds = model.predict(X_transformed)\n",
    "#         all_preds.append(preds)\n",
    "    \n",
    "#     # Weighted average (favor mid_scores model for test)\n",
    "#     weights = {\n",
    "#         'high_scores': 0.45,\n",
    "#         'mid_scores': 0.55,   # Higher weight for test distribution\n",
    "#         'low_scores': 0.0\n",
    "#     }\n",
    "    \n",
    "#     final_preds = np.zeros(len(X_test_enhanced))\n",
    "#     for i, task_name in enumerate(task_models.keys()):\n",
    "#         final_preds += weights[task_name] * all_preds[i]\n",
    "    \n",
    "#     return np.clip(final_preds, 0, 10)\n",
    "\n",
    "# y_test_pred = predict_multitask(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mahalanobis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:09:18.065385Z",
     "iopub.status.busy": "2025-11-18T18:09:18.064936Z",
     "iopub.status.idle": "2025-11-18T18:09:18.094900Z",
     "shell.execute_reply": "2025-11-18T18:09:18.093321Z",
     "shell.execute_reply.started": "2025-11-18T18:09:18.065255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from scipy.spatial.distance import mahalanobis\n",
    "# from sklearn.covariance import LedoitWolf\n",
    "\n",
    "# def add_mahalanobis_features(X_train, y_train):\n",
    "#     \"\"\"\n",
    "#     Add Mahalanobis distance to score bin centroids.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     bins_dict = {\n",
    "#         'low': (0, 6),      \n",
    "#         'mid_low': (6, 8),  \n",
    "#         'mid': (8, 9),      \n",
    "#         'high': (9, 11)  \n",
    "#     }\n",
    "#     centroids = {}\n",
    "#     inv_covs = {}\n",
    "    \n",
    "#     for bin_name, (low, high) in bins_dict.items():\n",
    "#         mask = (y_train >= low) & (y_train < high)\n",
    "#         X_bin = X_train[mask]\n",
    "        \n",
    "#         if X_bin.shape[0] < 3:\n",
    "#             # Not enough samples, skip this bin\n",
    "#             continue\n",
    "        \n",
    "#         centroids[bin_name] = X_bin.mean(axis=0)\n",
    "\n",
    "#         cov_estimator = LedoitWolf().fit(X_bin)\n",
    "#         cov = cov_estimator.covariance_\n",
    "        \n",
    "#         cov_reg = cov + np.eye(cov.shape[0]) * 1e-4\n",
    "        \n",
    "#         try:\n",
    "#             inv_covs[bin_name] = np.linalg.inv(cov_reg)\n",
    "#         except:\n",
    "#             print(f\"Warning: Singular covariance for {bin_name}, using pseudo-inverse\")\n",
    "#             inv_covs[bin_name] = np.linalg.pinv(cov_reg)\n",
    "\n",
    "#     return centroids, inv_covs\n",
    "\n",
    "# def calculate_mahal(X, centroids, inv_covs):\n",
    "#     mahal_features = []\n",
    "    \n",
    "#     for x in X:\n",
    "#         dists = []\n",
    "#         for bin_name in sorted(centroids.keys()):\n",
    "#             try:\n",
    "#                 dist = mahalanobis(x, centroids[bin_name], inv_covs[bin_name])\n",
    "#             except:\n",
    "#                 dist = np.linalg.norm(x - centroids[bin_name])\n",
    "#             dists.append(dist)\n",
    "        \n",
    "#         # Add inverse distances (closer = higher value)\n",
    "#         inv_dists = [1.0 / (d + 1e-6) for d in dists]\n",
    "        \n",
    "#         # Add both raw and inverse distances\n",
    "#         mahal_features.append(dists + inv_dists)\n",
    "\n",
    "#     return np.array(mahal_features)\n",
    "\n",
    "# mahal_train_features = np.hstack([\n",
    "#         train_embedding_features,\n",
    "#         train_tfidf.toarray(),\n",
    "# ])\n",
    "\n",
    "# centroids_mahal, inv_covs_mahal = add_mahalanobis_features(mahal_train_features, y_train)\n",
    "# mahal_train = calculate_mahal(mahal_train_features, centroids_mahal, inv_covs_mahal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:09:18.096272Z",
     "iopub.status.busy": "2025-11-18T18:09:18.096026Z",
     "iopub.status.idle": "2025-11-18T18:09:18.128825Z",
     "shell.execute_reply": "2025-11-18T18:09:18.127603Z",
     "shell.execute_reply.started": "2025-11-18T18:09:18.096254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# linear_grid = {\n",
    "#     \"LinearRegression\": {},\n",
    "#     \"Ridge\": {'alpha': [0.01, 0.1, 1, 10], 'max_iter': [None, 1000, 10000]},\n",
    "#     \"SGDRegressor\": {'penalty': ['l2', 'l1', 'elasticnet'], 'alpha': [1e-4, 1e-2, 1], 'max_iter': [100, 1000, 10000]},\n",
    "#     \"Lasso\": {'alpha': [0.01, 0.1, 1, 10], 'max_iter': [1000, 10000]},\n",
    "#     \"ElasticNet\": {'alpha': [0.01, 0.1, 1, 10], 'max_iter': [1000, 10000], 'l1_ratio': [0, 0.25, 0.5, 0.75, 1], \n",
    "#                    'selection': ['cyclic', 'random']}\n",
    "# }\n",
    "\n",
    "# linear_models = {\n",
    "#     \"LinearRegression\": LinearRegression(),\n",
    "#     \"Ridge\": Ridge(random_state= 42),\n",
    "#     \"SGDRegressor\": SGDRegressor(random_state= 42),\n",
    "#     \"Lasso\": Lasso(random_state= 42),\n",
    "#     \"ElasticNet\": ElasticNet(random_state= 42)\n",
    "# }\n",
    "\n",
    "# tree_grid = {\n",
    "#     \"DecisionTreeRegressor\": {'criterion': ['squared_error', 'friedman_mse'], 'splitter': ['best', 'random'], \n",
    "#                               'ccp_alpha': [0., 0.5, 1.], 'max_features': ['sqrt', 'log2', 1.0]},\n",
    "#     \"RandomForestRegressor\": {'n_estimators': [100, 200, 500], 'criterion': ['squared_error', 'friedman_mse'], \n",
    "#                               'ccp_alpha': [0., 0.5, 1.], 'max_features': ['sqrt', 'log2', 1.0]},\n",
    "#     \"AdaBoostRegressor\": {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1, 10], 'loss': ['linear', 'square', 'exponential']},\n",
    "# }\n",
    "\n",
    "# tree_models = {\n",
    "#     \"RandomForestRegressor\": RandomForestRegressor(random_state= 42),\n",
    "#     \"AdaBoostRegressor\": AdaBoostRegressor(random_state= 42),\n",
    "#     \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state= 42)\n",
    "# }\n",
    "\n",
    "# booster_grid = {\n",
    "#     \"LGBMRegressor\": {'boosting_type': ['gbdt', 'rf', 'dart'], 'learning_rate': [0.1, 0.01, 1], 'n_estimators': [100, 200, 500],\n",
    "#                      'reg_alpha': [0., 0.5, 1], 'reg_lambda': [0., 0.5, 1]},\n",
    "#     \"XGBRegressor\": {'n_estimators': [100, 200, 500], 'grow_policy': ['depthwise', 'lossguide'], 'learning_rate': [0.01, 0.1, 1],\n",
    "#                     'booster': ['gbtree', 'gblinear', 'dart'], 'reg_alpha': [0., 0.5, 1], 'reg_lambda': [0., 0.5, 1]},\n",
    "# }\n",
    "\n",
    "# booster_models = {\n",
    "#     \"LGBMRegressor\": LGBMRegressor(random_state= 42, verbosity= -1),\n",
    "#     \"XGBRegressor\": XGBRegressor(random_state= 42),\n",
    "# }\n",
    "\n",
    "# other_grid = {\n",
    "#     \"KNeighborsRegressor\": {'n_neighbors': [1, 3, 5, 7], 'weights': ['uniform', 'distance'], 'p': [1, 2, 1.5]},\n",
    "#     \"RadiusNeighborsRegressor\": {'radius': [1., 2., 1.5], 'weights': ['uniform', 'distance'], 'p': [1, 2, 1.5]},\n",
    "#     \"GaussianProcessRegressor\": {},\n",
    "#     \"MLPRegressor\": {'hidden_layer_sizes': [[16, 16], [8, 8, 8], [16, 8]], 'activation': ['relu', 'tanh', 'logistic', 'sigmoid'],\n",
    "#                     'alpha': [1e-4, 1e-3], 'learning_rate': ['constant', 'invscaling', 'adaptive'], 'learning_rate_init': [0.001, 0.01],\n",
    "#                     'max_iter': [200, 500, 1000]},\n",
    "#     \"SVR\": {'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto'], 'C': [0.01, 0.1, 1]}\n",
    "# }\n",
    "\n",
    "# other_models = {\n",
    "#     \"KNeighborsRegressor\": KNeighborsRegressor(),\n",
    "#     \"RadiusNeighborsRegressor\": RadiusNeighborsRegressor(),\n",
    "#     \"GaussianProcessRegressor\": GaussianProcessRegressor(random_state= 42),\n",
    "#     \"MLPRegressor\": MLPRegressor(random_state= 42),\n",
    "#     \"SVR\": SVR()\n",
    "# }\n",
    "\n",
    "# def run_grid_search(models_dict, grids_dict, X_train, y_train, cv= 5, n_jobs= -1, verbose= 0):\n",
    "#     \"\"\"\n",
    "#     Run GridSearchCV for the specified models in the dict on the specified params\n",
    "#     cv is the number of cross-validations\n",
    "#     \"\"\"\n",
    "\n",
    "#     requires_scaling = ['LinearRegression', 'Lasso', 'Ridge', 'ElasticNet', 'SGDRegressor',\n",
    "#                        'KNeighborsRegressor', 'RadiusNeighborsRegressor', 'SVR', 'MLPRegressor',\n",
    "#                        'GaussianProcessRegressor']\n",
    "#     results = []\n",
    "#     scorer = make_scorer(mean_squared_error, greater_is_better= False)\n",
    "#     scalers = {\n",
    "#         'StandardScaler': StandardScaler(),\n",
    "#         'MinMax': MinMaxScaler(),\n",
    "#         'MaxAbs': MaxAbsScaler()\n",
    "#     }\n",
    "\n",
    "#     for name, model in tqdm(models_dict.items(), desc= 'Progress'):\n",
    "#         tqdm.write(f'GridSearch for {name}')\n",
    "#         if name in requires_scaling:\n",
    "#             pipe = Pipeline([\n",
    "#                 ('scaler', StandardScaler()),\n",
    "#                 ('model', model)\n",
    "#             ])\n",
    "#             param_grid = {\n",
    "#                 **{f'scaler': list(scalers.values())},\n",
    "#                 **{f'model__{k}': v for k, v in grids_dict.get(name, {}).items()}\n",
    "#             }\n",
    "#         else:\n",
    "#             pipe = model\n",
    "#             param_grid = grids_dict.get(name, {})\n",
    "\n",
    "#         grid = GridSearchCV(estimator= pipe, param_grid= param_grid, cv= cv, \n",
    "#                             scoring= scorer, n_jobs= n_jobs, verbose= verbose)\n",
    "#         grid.fit(X_train, y_train)\n",
    "\n",
    "#         best_rmse = np.sqrt(-grid.best_score_)\n",
    "#         results.append({\n",
    "#             'model': name,\n",
    "#             'best_rmse': best_rmse,\n",
    "#             'best_params': grid.best_params_\n",
    "#         })\n",
    "#         tqdm.write(f'Best RMSE: {best_rmse:.4f}')\n",
    "#         tqdm.write(f'Best params: {grid.best_params_}')\n",
    "#         tqdm.write(f'\\n')\n",
    "\n",
    "#     return results\n",
    "\n",
    "\n",
    "# ## linear models\n",
    "# results_linear = run_grid_search(linear_models, linear_grid, X_train, y_train, cv= 5, n_jobs= -1, verbose= 0)\n",
    "\n",
    "\n",
    "## tree models\n",
    "# results_tree = run_grid_search(tree_models, tree_grid, X_train, y_train, cv= 5, n_jobs= -1, verbose= 0)\n",
    "\n",
    "## booster models\n",
    "# results_booster = run_grid_search(booster_models, booster_grid, X_train, y_train, cv= 5, n_jobs= -1, verbose= 0)\n",
    "\n",
    "\n",
    "## other models\n",
    "# results_other = run_grid_search(other_models, other_grid, X_train, y_train, cv= 5, n_jobs= -1, verbose= 0)\n",
    "\n",
    "\n",
    "# GridSearch for LinearRegression\n",
    "# Best RMSE: 0.9421\n",
    "# Best params: {'scaler': StandardScaler()}\n",
    "\n",
    "\n",
    "# GridSearch for Ridge\n",
    "# Best RMSE: 0.9420\n",
    "# Best params: {'model__alpha': 10, 'model__max_iter': None, 'scaler': MinMaxScaler()}\n",
    "\n",
    "\n",
    "# GridSearch for SGDRegressor\n",
    "# Best RMSE: 0.9418\n",
    "# Best params: {'model__alpha': 0.01, 'model__max_iter': 100, 'model__penalty': 'l1', 'scaler': StandardScaler()}\n",
    "\n",
    "\n",
    "# GridSearch for Lasso\n",
    "# Best RMSE: 0.9420\n",
    "# Best params: {'model__alpha': 0.01, 'model__max_iter': 1000, 'scaler': StandardScaler()}\n",
    "\n",
    "\n",
    "# GridSearch for ElasticNet\n",
    "# Best RMSE: 0.9419\n",
    "# Best params: {'model__alpha': 1, 'model__l1_ratio': 0, 'model__max_iter': 1000, 'model__selection': 'cyclic', 'scaler': StandardScaler()}\n",
    "\n",
    "\n",
    "# GridSearch for KNeighborsRegressor\n",
    "# Best RMSE: 0.9931\n",
    "# Best params: {'model__n_neighbors': 7, 'model__p': 1, 'model__weights': 'uniform', 'scaler': MaxAbsScaler()}\n",
    "\n",
    "\n",
    "# GridSearch for RadiusNeighborsRegressor\n",
    "# Best RMSE: 0.9420\n",
    "# Best params: {'model__p': 1, 'model__radius': 1.0, 'model__weights': 'uniform', 'scaler': MaxAbsScaler()}\n",
    "\n",
    "\n",
    "# GridSearch for GaussianProcessRegressor\n",
    "# Best RMSE: 1.1256\n",
    "# Best params: {'scaler': MinMaxScaler()}\n",
    "\n",
    "\n",
    "# GridSearch for MLPRegressor\n",
    "# Best RMSE: 0.9418\n",
    "# Best params: {'model__activation': 'logistic', 'model__alpha': 0.0001, 'model__hidden_layer_sizes': [16, 16], 'model__learning_rate': 'constant', 'model__learning_rate_init': 0.001, 'model__max_iter': 200, 'scaler': StandardScaler()}\n",
    "\n",
    "\n",
    "# GridSearch for SVR\n",
    "# Best RMSE: 0.9424\n",
    "# Best params: {'model__C': 1, 'model__gamma': 'scale', 'model__kernel': 'rbf', 'scaler': MinMaxScaler()}\n",
    "\n",
    "\n",
    "# GridSearch for RandomForestRegressor\n",
    "# Best RMSE: 0.9424\n",
    "# Best params: {'ccp_alpha': 0.5, 'criterion': 'squared_error', 'max_features': 'sqrt', 'n_estimators': 100}\n",
    "\n",
    "\n",
    "# GridSearch for AdaBoostRegressor\n",
    "# Best RMSE: 0.9427\n",
    "# Best params: {'learning_rate': 0.01, 'loss': 'exponential', 'n_estimators': 50}\n",
    "\n",
    "\n",
    "# GridSearch for DecisionTreeRegressor\n",
    "# Best RMSE: 0.9424\n",
    "# Best params: {'ccp_alpha': 0.5, 'criterion': 'squared_error', 'max_features': 'sqrt', 'splitter': 'best'}\n",
    "\n",
    "\n",
    "# GridSearch for LGBMRegressor\n",
    "# Best RMSE: 0.9446\n",
    "# Best params: {'boosting_type': 'gbdt', 'learning_rate': 0.01, 'n_estimators': 100, 'reg_alpha': 1, 'reg_lambda': 0.5}\n",
    "\n",
    "\n",
    "# GridSearch for XGBRegressor\n",
    "# Best RMSE: 0.9440\n",
    "# Best params: {'n_estimators': 200, 'grow_policy': 'lossguide', 'learning_rate': 0.01, 'booster': 'dart', 'reg_alpha': 1, 'reg_lambda': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:09:18.130324Z",
     "iopub.status.busy": "2025-11-18T18:09:18.129935Z",
     "iopub.status.idle": "2025-11-18T18:09:18.159127Z",
     "shell.execute_reply": "2025-11-18T18:09:18.157719Z",
     "shell.execute_reply.started": "2025-11-18T18:09:18.130300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_estimators': [30, 100, 300],\n",
    "#     'max_depth': [3, 5, 9],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'subsample': [0.7, 0.9, 1.0],\n",
    "#     'colsample_bytree': [0.7, 1.0],\n",
    "#     'scale_pos_weight': [ratio, ratio*1.5, ratio*2]\n",
    "# }\n",
    "\n",
    "# clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1, \n",
    "#                    colsample_bytree= 0.7, learning_rate= 0.1, max_depth= 5, n_estimators= 100, scale_pos_weight= ratio,\n",
    "#                    subsample= 0.9)\n",
    "# grid = GridSearchCV(clf, param_grid, scoring=scorer, cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "\n",
    "# grid.fit(X_train, is_rare)\n",
    "# best_model = grid.best_estimator_\n",
    "# print(grid.best_params_)\n",
    "# {'colsample_bytree': 0.7,\n",
    "#  'learning_rate': 0.1,\n",
    "#  'max_depth': 5,\n",
    "#  'n_estimators': 100,\n",
    "#  'scale_pos_weight': 10.52073732718894,\n",
    "#  'subsample': 0.9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification into rare and abundant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:09:18.161357Z",
     "iopub.status.busy": "2025-11-18T18:09:18.160800Z",
     "iopub.status.idle": "2025-11-18T18:09:18.190986Z",
     "shell.execute_reply": "2025-11-18T18:09:18.190019Z",
     "shell.execute_reply.started": "2025-11-18T18:09:18.161277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def modify_clf_train_data(X_train, y_train):\n",
    "# #     \"\"\"\n",
    "# #     Returns balanced X_resampled, y_resampled for classification.\n",
    "# #     \"\"\"\n",
    "# #     score_mult = {6: 4, 7: 2, 8: 0.8, 9: 0.06, 10: 0.125}\n",
    "# #     n = len(X_train)\n",
    "# #     X_list, y_list = [], []\n",
    "# #     for score, mult in score_mult.items():\n",
    "# #         mask = (y_train == score)\n",
    "# #         cty = int(mask.sum() * mult)\n",
    "# #         if score == 6 or score == 7:\n",
    "# #             X_majority, y_majority = X_train[mask == 0], y_train[mask == 0] \n",
    "# #             X_minority, y_minority = X_train[mask == 1], y_train[mask == 1]\n",
    "# #             X_ups, y_ups = resample(X_minority, y_minority, replace= True, n_samples = cty, random_state= 42)\n",
    "# #             X_list.append(X_ups)\n",
    "# #             y_list.append(y_ups)\n",
    "# #         else:\n",
    "# #             X_majority, y_majority = X_train[mask == 1], y_train[mask == 1] \n",
    "# #             X_minority, y_minority = X_train[mask == 0], y_train[mask == 0]\n",
    "# #             X_dps, y_dps = resample(X_majority, y_majority, replace= False, n_samples = cty, random_state= 42)\n",
    "# #             X_list.append(X_dps)\n",
    "# #             y_list.append(y_dps)\n",
    "\n",
    "# #     np.random.seed(200)\n",
    "# #     X_resamp, y_resamp = np.vstack(X_list), np.hstack(y_list)\n",
    "# #     shuffle_ids = np.random.permutation(len(X_resamp))\n",
    "# #     print(X_resamp.shape, y_resamp.shape)\n",
    "# #     return X_resamp[shuffle_ids], y_resamp[shuffle_ids]\n",
    "\n",
    "# X_resamp, y_resamp = modify_clf_train_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:09:18.192836Z",
     "iopub.status.busy": "2025-11-18T18:09:18.192552Z",
     "iopub.status.idle": "2025-11-18T18:09:18.219406Z",
     "shell.execute_reply": "2025-11-18T18:09:18.218209Z",
     "shell.execute_reply.started": "2025-11-18T18:09:18.192814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# is_rare = (y_train <= 7).astype(int)\n",
    "# is_abundant = (y_train >= 8).astype(int)\n",
    "# ratio = sum(is_rare == 0) / sum(is_rare == 1)\n",
    "# mask_rare = (is_rare == 1)\n",
    "# mask_abundant = (is_abundant == 1)\n",
    "\n",
    "# X_train_rare, y_train_rare = X_train[mask_rare], y_train[mask_rare]\n",
    "# X_train_abundant, y_train_abundant = X_train[mask_abundant], y_train[mask_abundant]\n",
    "\n",
    "\n",
    "# resamp_rare = (y_resamp <= 7).astype(int)\n",
    "# resamp_raremask = (resamp_rare == 1)\n",
    "# y_rare_resamp = y_resamp[resamp_raremask]\n",
    "\n",
    "\n",
    "# clf = GradientBoostingClassifier(random_state= 42, n_estimators= 300, max_depth= None)\n",
    "# # clf.fit(X_train, mask_rare)\n",
    "# clf.fit(X_resamp, resamp_raremask)\n",
    "\n",
    "# clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1, colsample_bytree= 0.7, \n",
    "#                     learning_rate= 0.1, max_depth= 5, n_estimators= 100, scale_pos_weight= ratio, subsample= 0.9)\n",
    "# clf.fit(X_train, is_rare, sample_weight= sample_weights)\n",
    "\n",
    "# probs = clf.predict_proba(X_train)[:, 1]\n",
    "# probs = clf.predict(X_resamp)\n",
    "# is_rare_pred = (probs > rare_thresh).astype(int)\n",
    "# print(classification_report(is_rare_pred, is_rare))\n",
    "\n",
    "# model_rare = RandomForestRegressor(random_state= 42, n_estimators= 300, criterion= 'squared_error', ccp_alpha= 0.5, max_features= 'sqrt')\n",
    "# model_abundant = RandomForestRegressor(random_state= 42, n_estimators= 100, criterion= 'squared_error', ccp_alpha= 0.5, max_features= 'sqrt')\n",
    "\n",
    "# model_rare.fit(X_train_rare, y_train_rare)\n",
    "# model_abundant.fit(X_train_abundant, y_train_abundant)\n",
    "\n",
    "# print(np.sqrt(mean_squared_error(y_train_rare, model_rare.predict(X_train_rare))))\n",
    "# print(np.sqrt(mean_squared_error(y_train_abundant, model_abundant.predict(X_train_abundant))))\n",
    "\n",
    "# def prediction_pipeline(X, clf, rare_thresh, model_rare, model_abundant):\n",
    "#     # probs = clf.predict_proba(X)[:, 1]\n",
    "#     probs = clf.predict(X)\n",
    "#     is_rare_pred = (probs > rare_thresh).astype(int)\n",
    "#     y_pred = np.zeros(X.shape[0])\n",
    "#     y_pred[is_rare_pred == 1] = model_rare.predict(X[is_rare_pred == 1])\n",
    "#     y_pred[is_rare_pred == 0] = model_abundant.predict(X[is_rare_pred == 0])\n",
    "#     return np.clip(y_pred, 0, 10)\n",
    "\n",
    "# y_train_pred = prediction_pipeline(X_train, clf, rare_thresh, model_rare, model_abundant)\n",
    "# print(np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
    "\n",
    "# y_test_predict = prediction_pipeline(X_test, clf, rare_thresh, model_rare, model_abundant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:09:18.220777Z",
     "iopub.status.busy": "2025-11-18T18:09:18.220510Z",
     "iopub.status.idle": "2025-11-18T18:09:18.249693Z",
     "shell.execute_reply": "2025-11-18T18:09:18.248139Z",
     "shell.execute_reply.started": "2025-11-18T18:09:18.220757Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Original Pipeline\n",
    "\n",
    "# # XGBRegressor\n",
    "# model1 = XGBRegressor(random_state= 42, n_estimators= 200, grow_policy= 'lossguide', learning_rate= 0.01, booster= 'dart', reg_alpha= 1, reg_lambda= 0.5, \n",
    "#                       objective='reg:pseudohubererror', huber_slope=2.0,\n",
    "#                      )\n",
    "# # MLPRegressor-> Standard Scaler needed\n",
    "# model3 = MLPRegressor(random_state= 42, activation= 'logistic', alpha= 0.0001, hidden_layer_sizes= [16, 16], learning_rate= 'constant', learning_rate_init= 0.001, max_iter= 200)\n",
    "# scaler = StandardScaler()\n",
    "# X_train_sc = scaler.fit_transform(X_train)\n",
    "# X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# model1.fit(X_train, y_train, sample_weight= sample_weights)\n",
    "# model2.fit(X_resamp, y_resamp)\n",
    "# model3.fit(X_train_sc, y_train, sample_weight= sample_weights)\n",
    "\n",
    "# # y_train_pred = model1.predict(X_train) \n",
    "# y_resamp_pred = model2.predict(X_resamp) \n",
    "# # y_train_pred = model3.predict(X_train_sc) \n",
    "# # y_test_predict = model1.predict(X_test)\n",
    "# # y_test_predict = model3.predict(X_test_sc)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14294892,
     "sourceId": 118082,
     "sourceType": "competition"
    },
    {
     "datasetId": 8642698,
     "sourceId": 13601043,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8744482,
     "sourceId": 13742982,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8744486,
     "sourceId": 13742988,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 487236,
     "modelInstanceId": 471331,
     "sourceId": 626123,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 488779,
     "modelInstanceId": 472908,
     "sourceId": 627969,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
